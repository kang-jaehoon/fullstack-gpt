{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3\n",
      "llama3\n",
      "a:  ğŸš€ As of now, there are eight planets officially recognized in our solar system:\n",
      "\n",
      "1. Mercury\n",
      "2. Venus\n",
      "3. Earth\n",
      "4. Mars\n",
      "5. Jupiter\n",
      "6. Saturn\n",
      "7. Uranus\n",
      "8. Neptune\n",
      "\n",
      "However, if you include dwarf planets and other smaller bodies like Pluto, Haumea, Makemake, Eris, and many others, the total number of celestial bodies in our solar system would be much higher! ğŸ¤¯\n",
      "b:  ğŸš€ According to NASA, there are currently eight planets that are officially recognized in our solar system:\n",
      "\n",
      "1. Mercury\n",
      "2. Mars\n",
      "3. Venus\n",
      "4. Earth\n",
      "5. Neptune\n",
      "6. Uranus\n",
      "7. Saturn\n",
      "8. Jupiter\n",
      "\n",
      "However, Pluto was previously considered a planet but is now classified as a dwarf planet.\n",
      "\n",
      "If we include exoplanets (planets outside our solar system), the number of known planets is over 4,000 and continues to grow as new discoveries are made! ğŸš€\n"
     ]
    }
   ],
   "source": [
    "## LLM, ChatModel ë‘ê°€ì§€ë¥¼ í˜¸ì¶œí•´ë³´ì.\n",
    "from langchain.llms.ollama import Ollama\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "llm = Ollama(model=\"llama3\")\n",
    "chat = ChatOllama(model=\"phi3:medium\")\n",
    "\n",
    "a = llm.predict(\"í–‰ì„±ì€ ëª‡ ê°œ ìˆë‚˜ìš”?\")\n",
    "b = chat.predict(\"í–‰ì„±ì€ ëª‡ ê°œ ìˆë‚˜ìš”?\")\n",
    "\n",
    "\n",
    "print(llm.model)\n",
    "print(chat.model)\n",
    "\n",
    "print(\"a: \", a)\n",
    "print(\"b: \", b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Nice to meet you!\\n\\nThe distance between Mexico and Thailand depends on the specific locations within each country. However, I can provide you with an approximate distance:\\n\\n* From Mexico City, Mexico to Bangkok, Thailand: approximately 14,500 kilometers (9,000 miles)\\n* From Cancun, Mexico to Phuket, Thailand: approximately 15,300 kilometers (9,500 miles)\\n\\nAs for my name, I'm Paolo, a geography expert!\" response_metadata={'model': 'llama3', 'created_at': '2024-07-06T11:17:54.348656Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 12341856959, 'load_duration': 6575572750, 'prompt_eval_count': 56, 'prompt_eval_duration': 389538000, 'eval_count': 93, 'eval_duration': 5374477000} id='run-db1c23b3-f1ca-43e0-bb0a-8f7b08651e62-0'\n"
     ]
    }
   ],
   "source": [
    "## ë©”ì‹œì§€ë“¤ì„ predictí•´ë³´ì.\n",
    "# HumanMessage: ìš°ë¦¬ê°€ ì•Œê³  ìˆê³ \n",
    "# AIMessage: AIì— ì˜í•´ ë³´ë‚´ì§\n",
    "# SystemMessage: LLMì— ì œê³µí•˜ê³  ì‹¶ì€ ì…‹íŒ…\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "chat = ChatOllama(\n",
    "  model=\"llama3\",\n",
    "  temperature=0.1\n",
    "  )\n",
    "\n",
    "messages = [\n",
    "  # \"ë‹¹ì‹ ì€ ì§€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì…¨ìŠµë‹ˆë‹¤.\"\n",
    "  SystemMessage(content=\"You are a geography expert. And you only reply in English.\"),\n",
    "  # ì•ˆë…•í•˜ì„¸ìš”, ì œ ì´ë¦„ì€ íŒŒì˜¬ë¡œì…ë‹ˆë‹¤!\n",
    "  AIMessage(content=\"Hello, My name is Paolo!\"),\n",
    "  # \"ë©•ì‹œì½”ì™€ íƒœêµ­ ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ì–¼ë§ˆì…ë‹ˆê¹Œ? ê·¸ë¦¬ê³  ë‹¹ì‹ ì˜ ì´ë¦„ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?\"\n",
    "  HumanMessage(content=\"What is the distance between Mexico and Thailand. Also, What is your name?\")\n",
    "]\n",
    "\n",
    "a = chat.predict_messages(messages)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The distance between Mexico and Thailand depends on the specific locations within each country. Here are the approximate distances between some major cities in Mexico and Thailand:\\n\\n* Mexico City, Mexico to Bangkok, Thailand: 14,444 kilometers (8,973 miles)\\n* Cancun, Mexico to Phuket, Thailand: 15,441 kilometers (9,593 miles)\\n* Tijuana, Mexico to Chiang Mai, Thailand: 14,931 kilometers (9,280 miles)\\n\\nTo give you a better idea, here are the distances between some major cities in Mexico and Thailand:\\n\\n* Mexico City, Mexico to Bangkok, Thailand: 14,444 km (8,973 mi)\\n* Guadalajara, Mexico to Chiang Mai, Thailand: 15,141 km (9,423 mi)\\n* Tijuana, Mexico to Phuket, Thailand: 15,441 km (9,593 mi)\\n\\nKeep in mind that these distances are approximate and may vary depending on the specific routes taken.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt templateë¥¼ í†µí•´ ë‹µë³€ì˜ ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆë‹¤.\n",
    "# LangChain frameworkì˜ í° ë¶€ë¶„ì´ promptì— ì§‘ì¤‘ë˜ì–´ ìˆê¸°ë„ í•˜ë‹¤.\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"What is the distance between {country_a} and {country_b}\")\n",
    "\n",
    "chat = ChatOllama(\n",
    "  model=\"llama3\",\n",
    "  temperature=0.1\n",
    "  )\n",
    "\n",
    "# formatì€ placeholder ì‹¤ì œ ê°’ìœ¼ë¡œ ì¹˜í™˜í•œ ë¬¸ìì—´ì„ ë³´ì—¬ì¤€ë‹¤.\n",
    "prompt = template.format(country_a=\"Mexico\", country_b=\"Thailand\")\n",
    "\n",
    "chat.predict(prompt)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  [SystemMessage(content='You are a geography expert. And you only reply in English.'), AIMessage(content='Hello, My name is Paolo!'), HumanMessage(content='What is the distance between Mexico and Thailand. Also, what is your name?')]\n",
      "answer:  content=\"Nice to meet you!\\n\\nThe distance between Mexico and Thailand depends on the specific locations within each country. However, I can give you an approximate distance:\\n\\n* The distance from Mexico City, Mexico (19.4326Â° N, 99.1333Â° W) to Bangkok, Thailand (13.7564Â° N, 100.5018Â° E) is approximately 18,444 kilometers (11,468 miles).\\n* The distance from Cancun, Mexico (21.1667Â° N, 86.8333Â° W) to Phuket, Thailand (7.8572Â° N, 98.3333Â° E) is approximately 17,441 kilometers (10,843 miles).\\n\\nAs for my name, I'm Paolo, a geography expert!\" response_metadata={'model': 'llama3', 'created_at': '2024-07-06T14:13:16.658756Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 9049508792, 'load_duration': 3092667, 'prompt_eval_count': 28, 'prompt_eval_duration': 337782000, 'eval_count': 160, 'eval_duration': 8705175000} id='run-a47ce7d6-6ce2-4ac0-b45e-83b545450c5f-0'\n"
     ]
    }
   ],
   "source": [
    "# ì¢€ë” ì‰½ê²Œ ì‘ì„±í•´ë³´ì.\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "chat = ChatOllama(\n",
    "  model=\"llama3\",\n",
    "  temperature=0.1\n",
    "  )\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a geography expert. And you only reply in {language}.\"),\n",
    "  (\"ai\", \"Hello, My name is {name}!\"),\n",
    "  (\"human\", \"What is the distance between {country_a} and {country_b}. Also, what is your name?\")\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(language=\"English\", name=\"Paolo\", country_a=\"Mexico\", country_b=\"Thailand\")\n",
    "\n",
    "question = prompt\n",
    "answer = chat.predict_messages(question)\n",
    "\n",
    "print(\"question: \", question)\n",
    "print(\"answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "  \n",
    "  def parse(self, text):\n",
    "    # ì–‘ìª½ ê³µë°±ì„ ì œê±°í•œ í›„ ë°˜í™˜. \",\"ë¡œ êµ¬ë¶„í•˜ì—¬ listë¡œ ë³€í™˜\n",
    "    items = text.strip().split(\",\")\n",
    "    return list(map(str.strip, items))\n",
    "  \n",
    "p = CommaOutputParser()\n",
    "\n",
    "p.parse(\"Hello, how, are, you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['black', 'white', 'red', 'green', 'blue']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "chat = ChatOllama(\n",
    "  model=\"llama3\",\n",
    "  temperature=0.1\n",
    "  )\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a list generating machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase. Do NOT reply with anyting else.\"),\n",
    "  (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "  max_items=\"5\",\n",
    "  question=\"What are the colors?\"\n",
    "  )\n",
    "\n",
    "# prompt\n",
    "result = chat.predict_messages(prompt)\n",
    "\n",
    "p = CommaOutputParser()\n",
    "\n",
    "p.parse(result.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
